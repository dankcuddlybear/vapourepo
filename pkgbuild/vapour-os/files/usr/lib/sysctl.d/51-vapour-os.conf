######## [ INTEL GRAPHICS ] ########
## Disable paranoid (which fills log with perf messages)
dev.i915.perf_stream_paranoid=0



######## [ KERNEL ] ########
## Enable Magic Sysrq functions
# 0  - disable sysrq completely
# 1  - enable all functions of sysrq
# >1 - bitmask of allowed sysrq functions (see below):
#   2 =   0x2 - enable control of console logging level
#   4 =   0x4 - enable control of keyboard (SAK, unraw)
#   8 =   0x8 - enable debugging dumps of processes etc.
#  16 =  0x10 - enable sync command
#  32 =  0x20 - enable remount read-only
#  64 =  0x40 - enable signalling of processes (term, kill, oom-kill)
# 128 =  0x80 - allow reboot/poweroff
# 256 = 0x100 - allow nicing of all RT tasks
kernel.sysrq=1

## Disable: kexec, unpriviliged BPF programs, unprivileged user namespace usage,
kernel.kexec_load_disabled = 1
kernel.printk = 3 3 3 3
kernel.unprivileged_bpf_disabled=1
kernel.unprivileged_userns_clone=0

## Hide kernel symbol addresses in /proc/kallsyms from regular users without CAP_SYSLOG
kernel.kptr_restrict = 1

## Increase the sched_rt_runtime_us to mitigate issues:
## sched: RT throttling activated
kernel.sched_rt_runtime_us=980000



######## [ NETWORK ] ########
## BPF hardening (priviliged and unpriviliged)
net.core.bpf_jit_harden = 2

## Allow TCP fast open for incoming/outgoing connections to reduce latency
net.ipv4.tcp_fastopen = 3

## Reuse sockets in TIME-WAIT state for new connections
net.ipv4.tcp_tw_reuse = 1

## Close socket after waiting 10 seconds for a final FIN packet
net.ipv4.tcp_fin_timeout = 10

## Whether TCP should start at the default window size
## only for new connections or also for existing connections
## that have been idle for too long.
## This setting kills persistent single connection performance
## and should be turned off:
net.ipv4.tcp_slow_start_after_idle = 0

## After 5 seconds of idle (time), send keepalive probe
## every 5 seconds (intvl) and up to 5 times (probes)
## This will detect dead connections after 30 seconds.
net.ipv4.tcp_keepalive_time = 5
net.ipv4.tcp_keepalive_intvl = 5
net.ipv4.tcp_keepalive_probes = 5

## Enable MTU probing
net.ipv4.tcp_mtu_probing = 1

## BBR congestion control
net.core.default_qdisc = cake
net.ipv4.tcp_congestion_control = bbr2

## Increase Ephemeral port range (RFC 6056 recommends 1024-65536)
net.ipv4.ip_local_port_range = 1024 65535

## TCP SYN cookie protection
net.ipv4.tcp_syncookies = 1

## Drop RST packets for sockets in the time-wait state.
## Protects against time-wait assasination hazards.
net.ipv4.tcp_rfc1337 = 1

## Disable ICMP redirect acceptance
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.all.secure_redirects = 0
net.ipv4.conf.default.secure_redirects = 0
net.ipv6.conf.all.accept_redirects = 0
net.ipv6.conf.default.accept_redirects = 0



######## [ VM ] ########
## Total available free/reclaimable memory pages before a process
## generating disk writes starts writing out dirty data.
## Higher ratios may increase performance but increases risk of data loss
## and increases writeback time on spinning disks.
## Lower ratios may cause higher latency.
## A reasonable value is 128MiB (134217728 bytes) for foreground
## and 64MiB (67108864 bytes) for background
vm.dirty_bytes = 134217728
vm.dirty_background_bytes = 67108864

## Controls tendency of kernel to reclaim VFS cache memory.
## Lowering may improve responsiveness.
## 200: Always reclaim
## 100: Equally inclined whether or not to reclaim
## Do not set to zero, may produce out-of-memory conditions
vm.vfs_cache_pressure = 25

## Wake kernel flusher threads every 5000 centiseconds (5 seconds)
## to write dirty data. 0: Disable periodic writeback
vm.dirty_writeback_centisecs = 5000

## Delay in seconds before spinning up HDD to write dirty data.
## Increases power savings but also increases risk of losing unsaved work.
vm.laptop_mode = 60

## Avoid swapping unless system has no more free memory
vm.swappiness = 5

## Disable zone reclaim as it introduces latency spikes
vm.zone_reclaim_mode = 0

## Disaable proactive compaction as it causes latency
vm.compaction_proactiveness = 0

## Reduce maximum page lock acquisition latency while retaining
## adequate throughput
vm.page_lock_unfairness = 1
